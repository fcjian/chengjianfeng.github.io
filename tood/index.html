<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WJFX2BFB9X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-WJFX2BFB9X');
</script>
	<title>TOOD: Task-aligned One-stage Object Detection</title>
	<meta property="og:image" content="./resources/overview.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="TOOD: Task-aligned One-stage Object Detection" />
	<meta property="og:description" content="C. Feng, Y Zhong, Y. Gao, M. Scott, W. Huang" />
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">TOOD: Task-aligned One-stage Object Detection</span>
		<br>
		<br>
		<table align=center width=1000px>
			<table align=center width=1000px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="https://fcjian.github.io/">
							Chengjian Feng</a> *</span>
							<br>
							<span style="font-size:22px">Intellifusion Inc.</span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="https://y-zhong.info/">
							Yujie Zhong</a> *</span>
							<br>
							<span style="font-size:22px">Meituan Inc.</span>
						</center>
					</td>					
					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="">
							Yu Gao</a></span>
							<br>
							<span style="font-size:22px">ByteDance Inc.</span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=500px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="">
							Matthew R. Scott</a></span>
							<br>
							<span style="font-size:22px">Malong LLC</span>
						</center>
					</td>

					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="http://whuang.org/">
							Weilin Huang</a><sup> <img class="round" style="width:20px" src="./resources/email_flag.png"/></sup></span>
							<br>
							<span style="font-size:22px">Alibaba Group</span>
						</center>
					</td>
				</tr>
			</table>

			<br>
			<table align=center width=500px>
				<tr>
					<td align=center width=500px>
						<center>
							<span style="font-size:22px">
								<em>ICCV</em> 2021
								<font color="red"><strong>(Oral)</strong></font>
							</span>
						</center>
					</td>
				</tr>
			</table>
			
			<table align=center width=250px>
				<br>
				<tr>
					<span style="font-size:22px">
						<a href="https://arxiv.org/abs/2108.07755">ArXiv</a> |
						<a href="https://github.com/fcjian/TOOD">Code</a> |
						<a href="./resources/bibtex.txt">Bibtex</a><br>
					</span>
				</tr>
			</table>
		</table>
	</center>
	<br>
	<center>
		<table align=center width=1000px>
			<tr>
				<td width=1000px>
					<center>
						<img src="./resources/overview.png" alt="clean-usnob" width="550" height="450">
					</center>
				</td>
			</tr>
		</table>
		<!--
		<table align=center width=850px>
			<tr>
				<td>
					This was a template originally made for <a href="http://richzhang.github.io/colorization/">Colorful Image Colorization</a>. The code can be found in this <a href="https://github.com/richzhang/webpage-template">repository</a>.
				</td>
			</tr>
		</table>
	-->
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				One-stage object detection is commonly implemented by optimizing two sub-tasks: object classification and localization, using heads with two parallel branches, which might lead to a certain level of spatial misalignment in predictions between the two tasks. In this work, we propose a Task-aligned One-stage Object Detection (TOOD) that explicitly aligns the two tasks in a learning-based manner. First, we design a novel Task-aligned Head (T-Head) which offers a better balance between learning task-interactive and task-specific features, as well as a greater flexibility to learn the alignment via a task-aligned predictor. Second, we propose Task Alignment Learning (TAL) to explicitly pull closer (or even unify) the optimal anchors for the two tasks during training via a designed sample assignment scheme and a task-aligned loss. Extensive experiments are conducted on MS-COCO, where TOOD achieves a <b>51.1 AP</b> at single-model single-scale testing. This surpasses the recent one-stage detectors by a large margin, such as ATSS (47.7 AP), GFL (48.2 AP), and PAA (49.0 AP), with fewer parameters and FLOPs. Qualitative results also demonstrate the effectiveness of TOOD for better aligning the tasks of object classification and localization.
			</td>
		</tr>
	</table>
	<br>
	
	<hr>
	<center><h1>Results</h1></center>

	<table align=center width=750px>
		<center>
			<tr>
				<td>
				<center>
					<td><img class="round" style="width:750px" src="./resources/quantitative_result.png"/></td>
				</center>
				</td>
			</tr>
		</center>
	</table>
	<br>

	<hr>
	<center><h1>Visualizations</h1></center>

	<table align=center width=750px>
		<center>
			<tr>
				<td>
				<center>
					<td><img class="round" style="width:1000px" src="./resources/qualitative_result.png"/></td>
				</center>
				</td>
			</tr>
		</center>
	</table>
	<br>
	<!--
	<hr>
	<center><h1>Video</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/embed/ZTn0xRJvndU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p>
	
	
	<hr>
	
	<center><h1>Results</h1></center>

	<table align=center width=750px>
		<center>
			<tr>
				<td>
				<center>
					<td><img class="round" style="width:750px" src="./resources/graph.png"/></td>
				</center>
				</td>
			</tr>
		</center>
	</table>
	<br>
	<table align=center width=1000px>
		<tr>
			<td>
	<strong>Results comparison on DAVIS2016.</strong>
	Note that, supervised approaches may use models pretrained on ImageNet,
	but here we only count number of images with pixel-wise annotations.
			</td>
		</tr>
	</table>

	<br><br>
	<table align=center width=1000px>
		<center>
			<tr>
				<td>
				<center>
					<td><img class="round" style="width:1000px" src="./resources/main.png"/></td>
				</center>
				</td>
			</tr>
		</center>
	</table>
	<br>
	<table align=center width=1000px>
		<tr>
			<td>
	<strong>Full comparison on unsupervised video segmentation.</strong>
	We consider three popular datasets, DAVIS2016, SegTrack-v2 (STv2), and FBMS59.
	Models above the horizontal dividing line are trained without using any manual annotation,
	while models below are pre-trained on image or video segmentation datasets, e.g. DAVIS, YouTube-VOS,
	thus requiring ground truth annotations at training time.
	Numbers in parentheses denote the additional usage of significant post-processing, 
	e.g. multi-step flow, multi-crop, temporal smoothing, CRFs.
			</td>
		</tr>
	</table>

	<br><br>
	<table align=center width=1000px>
		<center>
			<tr>
				<td>
				<center>
					<td><img class="round" style="width:1000px" src="./resources/moca.png"/></td>
				</center>
				</td>
			</tr>
		</center>
	</table>
	<br>
	<table align=center width=1000px>
		<tr>
			<td>
	<strong>Comparison results on MoCA dataset.</strong>
	We report the successful localization rate for various thresholds.
	Both CIS and Ours were pre-trained on DAVIS and finetuned on MoCA in a self-supervised manner.
	Note that, our method achieves comparable Jaccard to MATNet (2nd best model on DAVIS), 
	without using RGB inputs and without any manual annotation for training.
			</td>
		</tr>
	</table>


	
	<br><br>
	<hr>
	
	<center><h1>Visualizations</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>

				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=200px>
				<center>
					<td><img class="round" style="width:200px" src="./resources/flatfish_1.gif"/></td>
				</center>
			</td>
			<td align=center width=200px>
				<center>
					<td><img class="round" style="width:200px" src="./resources/parachute.gif"/></td>
				</center>
			</td>
			<td align=center width=200px>
				<center>
					<td><img class="round" style="width:200px" src="./resources/spider_tailed_horned_viper_3.gif"/></td>
				</center>
			</td>
			<td align=center width=200px>
				<center>
					<td><img class="round" style="width:200px" src="./resources/soapbox.gif"/></td>
				</center>
			</td>
			<td align=center width=200px>
				<center>
					<td><img class="round" style="width:200px" src="./resources/pygmy_seahorse_2.gif"/></td>
				</center>
			</td>
		</tr>
	</table>

	<table align=center width=400px>
		<tr>
			<td align=center width=200px>
				<center>
					<td><img class="round" style="width:200px" src="./resources/paragliding-launch.gif"/></td>
				</center>
			</td>
			<td align=center width=200px>
				<center>
					<td><img class="round" style="width:200px" src="./resources/frog.gif"/></td>
				</center>
			</td>
			<td align=center width=200px>
				<center>
					<td><img class="round" style="width:200px" src="./resources/drift.gif"/></td>
				</center>
			</td>
			<td align=center width=200px>
				<center>
					<td><img class="round" style="width:200px" src="./resources/snow_leopard_2.gif"/></td>
				</center>
			</td>
			<td align=center width=200px>
				<center>
					<td><img class="round" style="width:200px" src="./resources/car-roundabout.gif"/></td>
				</center>
			</td>
		</tr>
	</table>
	-->
	<table align=center width=850px>
		<center>
			<tr>
				<!--
				<td>
					Coming soon...
				</td>
			-->
			</tr>
		</center>
	</table>
	<!--
	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/charigyang/motiongrouping'>[GitHub]</a>
			</center>
		</span>
	</table>
	<br>
-->

	<hr>
	<table align=center width=800px>
		<center><h1>Publication</h1></center>
		<tr>
			<td><a href="https://arxiv.org/pdf/2108.07755.pdf"><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">C. Feng, Y Zhong, Y. Gao, M. Scott, W. Huang<br>
				<b>TOOD: Task-aligned One-stage Object Detection</b><br>
				<em>ICCV</em> 2021
				<font color="red"><strong>(Oral)</strong></font>
				<br>
				<a href="https://arxiv.org/abs/2108.07755">ArXiv</a> |
				<a href="https://github.com/fcjian/TOOD">Code</a> |
				<a href="./resources/bibtex.txt">Bibtex</a><br>
				<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>
	<br>
	<!--
	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>-->

	<!--
	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					We thank Yimeng Long for assistance on data annotation, Joao Carreira for an interesting discussion,
					Guanqi Zhan, Ragav Sachdeva, K R Prajwal, and Aleksandar Shtedritski for proofreading.
					This research is supported by the UK EPSRC CDT in AIMS (EP/S024050/1), a Royal Society Research Professorship, and the UK EPSRC Programme Grant Visual AI (EP/T028572/1).<br>
					<br>

					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>
	-->

	<table style="width:95%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr>
		<td style="padding:0px">
		  <br>
		  <p style="text-align:right;font-size:small;">
			Webpage template modified from <a href="https://github.com/richzhang/webpage-template/">here</a>.
		  </p>
		</td>
	  </tr>
	</tbody></table>

<br>
</body>
</html>

